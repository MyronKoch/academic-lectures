---
marp: true
class: center
---

<!--
_slide: bg
class: lead
-->
![bg](./images/MLK_AI_Background.png)

---

<!-- _comment:
*   "THANKS TO PRINCETON UNIVERSITY FOR HOSTING US. 
THANKS TO ANGELA (or "moontrips") FOR IDENTIFYING THIS OPPORTUNITY, AND FOR ALL THE GREAT WORK SHE'S DONE WITH ANDROMEDA AND WITH WEB3 UNIVERSITY.

PLEASE TAKE A MOMENT TO SCAN THE QR CODES TO CONNECT WITH W3U ON TWITTER AND GITHUB.
-->

![Connect with Web3 University QR Codes](./images/Web3U_QR_codes.png)

---

# Introduction: The AI Development Landscape

<!-- _comment:
*   "Welcome everyone. Today, we're diving into the rapidly evolving world of AI development."
*   "We'll cover the essential concepts, the latest tools, and look at how these powerful technologies, especially large language models, are being built and applied, with a special focus on the intersection with emerging Web3 platforms."
-->

*   **Today's Agenda:** Core AI concepts, Frontier Models, Development Tools, AI x Web3 Intersection (Focus: Andromeda Protocol), Future Trends.
*   To follow along on the extended Resources document, go to http://openexamples.com ....I'll be referencing specific tables and figures throughout the talk.

---

## "Slow at first, then suddenly...." 
# The AI Revolution is Here

<!-- _comment:
*   "It's undeniable – we're in the midst of an AI revolution, particularly driven by advancements in Large Language Models or LLMs."
*   "Think about tools like ChatGPT, Claude, Gemini – these aren't just incremental improvements; they represent a fundamental shift in how we interact with computers and information."
*   "These models can understand and generate human-like text, code, images, and more, impacting almost every field."
*   *(Optional: Mention a recent, relatable AI news headline or application).*
-->

*   **Key Drivers:** Transformer Architecture (Google, 2017), Massive Datasets, Compute Power (GPUs and especially TPUs). 
*    Soon, Nvidia's "Spark" PCs will put TPUs on consumers desk, for under $5k

---

# Why AI x Web3 Matters

<!-- _comment:
*   "A particularly exciting frontier is the convergence of AI and Web3 – technologies like blockchain, decentralized networks, and crypto."
*   "This isn't just theoretical. We're seeing real platforms emerge, aiming to build AI-native decentralized systems. We'll explore some examples including Andromeda Protocol shortly.." 
-->

**[[Link: Andromeda Protocol]](https://andromedaprotocol.io) [[Link: Bittensor]](https://bittensor.com) [[Link: Akash]](https://akash.network) [[Link: SingularityNET]](https://singularitynet.io) [[Link: ChainGPT]](https://www.chaingpt.org)**

*   **Synergies:**
    *   AI enhancing Web3: Smart contract generation, DAO governance assistance, automated agent interactions on-chain, intuitive user interfaces for complex protocols.
    *   Web3 enhancing AI: Decentralized compute/storage for training/inference (e.g., Bittensor, Akash), verifiable credentials for AI models/outputs, data provenance, new incentive models for AI development.

---

# A Brief History - Key Milestones
### > https://ai-timeline.org/
<!-- _comment:
*   "LOOK AT IT GO!! To understand where we are, let's quickly touch upon some pivotal moments." *(Referencing Section 1 of the Markdown resource)*
*   "The pace of progress, especially since the Transformer paper, has been exponential, trending parabolic. What took decades is now happening in months or even days. This will increase further with better models and faster computing."
*   **(Visual Aid):** *(Simple timeline graphic, referencing `ai-timeline.org`)* [[Link: AI Timeline]](https://ai-timeline.org/)
-->

*   **Highlight 3-4 Key Moments:**
    *   **1956:** Dartmouth Workshop (Birth of AI term)
    *   **2012:** AlexNet (Deep Learning breakthrough in vision)
    *   **2017:** "Attention Is All You Need" (Transformer architecture) [[Link: Paper]](https://arxiv.org/abs/1706.03762)
    *   **2022/2023:** ChatGPT/GPT-4 (Mainstream adoption, emergent reasoning)

---

# Focus Platform: Andromeda Protocol

<!-- _comment:
*   "Now, let's transition to a concrete example of a platform operating at this exciting intersection of AI and Web3 – Andromeda Protocol."
*   "The analogy to how tools simplified traditional web development is apt – lowering the barrier to entry and speeding up innovation."
-->

*   **What is Andromeda?** A Web3 Operating System built on Cosmos SDK for simpler cross-chain dApp development. [[1]](https://app.andromedaprotocol.io/flex-builder)
*   **Core Value Prop:** Simplify building cross-chain dApps, reducing need for deep smart contract expertise.
*   **How? ADOs:** Uses Andromeda Digital Objects (ADOs) - composable, pre-built smart contract modules (like digital LEGOs) for faster development. [[2]](https://docs.andromedaprotocol.io/andromeda/andromeda-digital-objects/introduction-to-ados)

---

# Bridging Web3 and AI on Andromeda (1/3)

<!-- _comment:
*   "So where does AI fit into this Web3 OS?"
*   "This moves beyond just using AI *off-chain* to analyze blockchain data; it's about making the *on-chain* environment and the *development experience* itself smarter."
*   **(Visual Aid):** *(Speaker refers to slide/points)* "Again, you can explore the main site ([andromedaprotocol.io](https://andromedaprotocol.io)) or check out some demo videos for a visual sense." *(Links to specific demos if available)* [[4]](https://www.youtube.com/watch?v=u1XkLzbIzB0) [[5]](https://www.youtube.com/watch?v=14yPeonB2P4)
-->

*   **The Opportunity:** Embed AI into the platform to help users and enable AI-driven on-chain logic.
*   **Feature 1: AI-Assisted Onboarding:** Guided, conversational AI to help new users understand concepts, use ADOs, and navigate the ecosystem.
*   **Vision:** Seamless blend of AI and Web3 to make decentralized tech accessible and easier to harness.

---


# AI on Andromeda (2/3)

<!-- _comment:
*   "Continuing with the AI features..."
*   "Like all AI features in every app everywhere, this is a work in progress, and Browser-Use is a brand-new and experimental technology that might, quite frankly, be a bit buggy, and furthermore, might be bested by a newer way to automate control of the browser."
-->

*   **Feature 2: AI Building Assistant (Browser Extension):**
    *   Proactive build/configuration partner.
    *   Observes context in web app/docs.
    *   Uses *your own* API keys (privacy/control).
    *   Can suggest ADOs, help configure, generate code snippets (CLI), anticipate next steps.
    *   *(Experimental - may have bugs, browser automation tech evolving).* [[3]](https://docs.andromedaprotocol.io/andromeda/andromeda-cli/introduction)

---
# AI on Andromeda (3/3)

<!-- Here, why don't you take a look at our new onboarding platform? It's in beta, and not all the features are there. You can see a spot where the Chrome extension will be downloadable.
-->

Click this QR code and answer the few questions to get a custom onboarding path for Andromeda. [onboarding platform](https://oop-1-omega.vercel.app/)

![OOP](./images/OOP_beta_QR_for_Princeton.png)
[onboarding platform](https://oop-1-omega.vercel.app/)
---

# AI & LLM Fundamentals: Under the Hood

<!-- _comment:
*   "Okay, we've seen the impact of AI and looked at a specific platform, Andromeda. Now, let's get a bit more technical and understand the core concepts driving these large language models."
*   "The single most important breakthrough in recent years is the **Transformer architecture**. Introduced in 2017, it revolutionized how machines process language."
-->

*   **Focus:** Understanding the Transformer and key model architectures that power today's AI.

---

# The Transformer: "Attention Is All You Need"

<!-- _comment:
*   "Before Transformers, models processed text sequentially (like RNNs or LSTMs), which struggled with long-range dependencies – understanding how words far apart in a sentence relate."
*   "The Transformer introduced the concept of **'Attention'**. Imagine the model looking at a word and asking, 'Which other words in this sentence are most relevant to understanding *this* word?' It can weigh the importance of all other words simultaneously, regardless of distance." [[Link: Attention Paper]](https://arxiv.org/abs/1706.03762)
*   **(Visual Aid):** "There are fantastic visual explainers online that really help build intuition for this." [[Link: Transformer Explainer]](https://poloclub.github.io/transformer-explainer/) [[Link: LLM 3D Walkthrough]](https://bbycroft.net/llm) "Highly recommend checking these out later."
*   "This parallel processing and attention mechanism is why Transformers scale so well and form the backbone of models like GPT, Claude, Gemini, Llama, etc."
-->

*   **Core Idea:** Self-Attention Mechanism allows models to weigh the importance of different words in the input text relative to each other, capturing context more effectively than sequential methods.

---

# Model Architectures: A Cheat Sheet

<!-- _comment:
*   "While most frontier models use the Transformer base, there are different 'flavors' or architectures built upon it, each with trade-offs." *(Referencing Section 2.2 Table)*
*   "You don't need to memorize all these, but understanding the Dense vs. MoE distinction is key to understanding why some models are faster or have different capabilities."
*   *(Optional: Briefly mention State-Space Models like Mamba as a non-Transformer alternative for very long context, referencing Section 2.3)* "There are also newer architectures like State-Space Models aiming for even longer context windows, but Transformers remain dominant for now."
-->
https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0681d04f-0cd6-45a7-b1f1-a37f9269d01d_1116x1126.gif
*   **Main Types:**
    *   **Dense Transformer:** Classic (GPT-3/4). All parts active. Robust but expensive.
    *   **Mixture-of-Experts (MoE):** Routes tokens to specialist sub-networks (e.g., Mixtral, Gemini 1.5). High param count, cheaper compute. Complex routing.
    *   **Hybrid:** Blends Dense & MoE (e.g., Claude 3.7 Sonnet). Aims for best of both.

---

# Beyond Text: Multimodal Models

<!-- _comment:
*   "While text was the initial focus, today's frontier models are increasingly **multimodal**, meaning they can understand and generate different types of data, like images, audio, and even video." *(Referencing Section 2.5 Table)*
*   "This opens up vast new possibilities for creative work, accessibility, and complex problem-solving."
-->

*   **Vision:** Models (GPT-4o, Gemini) 'see' images/screenshots/docs to answer questions, analyze charts, debug code. [[Link: GPT-4o]](https://openai.com/index/gpt-4o-system-card/) [[Link: Gemini]](https://deepmind.google/technologies/gemini/)
*   **Image/Video Gen:** MidJourney (stylized images), RunwayML (image & video, e.g., Gen-4). [[Link: MidJourney]](https://www.midjourney.com) [[Link: RunwayML]](https://runwayml.com)
*   **Music Gen:** Suno, Udio create songs (vocals, instruments) from prompts. [[Link: Suno]](https://suno.com) [[Link: Udio]](https://www.udio.com)
*   **Voice Synthesis/Cloning:** ElevenLabs synthesizes speech, clones voices. [[Link: ElevenLabs]](https://elevenlabs.io/)
*   **Trend:** AI becoming general-purpose engine across modalities.

---

# Advanced Technique: Retrieval-Augmented Generation (RAG)

<!-- _comment:
*   "Now that we understand the basics of how these models work, let's look at some advanced techniques for *using* them effectively. One of the most impactful is Retrieval-Augmented Generation, or RAG."
*   "Think about this: LLMs are trained on vast but ultimately static datasets. They don't inherently know about *your* specific project documents, the latest company news, or real-time information."
*   **(Analogy):** "It's like giving the LLM an 'open book' exam specific to your question, rather than relying solely on its memorized knowledge."
*   **(Visual Aid):** *(Simple diagram: User Query -> Retrieval Step (finds relevant docs) -> Docs + Query -> LLM -> Grounded Answer)*
-->

*   **The Problem:** Base LLMs lack access to private/real-time data and can "hallucinate".
*   **The Solution (RAG):** First *retrieve* relevant external info, then give context + query to LLM for a grounded answer.

---

# RAG Flavors: Tailoring Knowledge Retrieval (1/2)

<!-- _comment:
*   "RAG isn't one-size-fits-all. Depending on the type of data you're working with and the kind of questions you need to answer, different RAG approaches can be more effective." *(Referencing Section 3.1 Table)*
*   "Let's look at the first few common variants:"
-->

*   **Examples:**
    *   **Plain RAG:** Simple vector search over text chunks. Good for general Q&A.
    *   **Graph RAG:** Builds/traverses a knowledge graph (code functions, story characters) before retrieval. Great for multi-step reasoning, codebases.
    *   **Hybrid RAG:** Combines keyword search (BM25) + vector search. Useful for exact terms (legal, medical).

---

# RAG Flavors: Tailoring Knowledge Retrieval (2/2)

<!-- _comment:
*   "Continuing with more RAG variants..."
*   "The key takeaway is that the 'Retrieval' part of RAG is highly customizable to best suit the knowledge source and the task."
-->

*   **More Examples:**
    *   **Hierarchical RAG:** Retrieves broad sections (chapters), then drills into sub-chunks. Ideal for long docs (textbooks, manuals).
    *   **Agentic/Tool RAG:** Retrieval step is part of an agent using other tools (calculators, APIs). Enables dynamic workflows ('lookup -> calculate').
    *   **Multimodal RAG:** Retrieves relevant images, audio, or video alongside text.

---

# Advanced Technique: Prompt Engineering 101

<!-- _comment:
*   "Beyond feeding models *context* with RAG, *how* we ask questions – the **prompt** – significantly impacts the quality and structure of the answer. This is Prompt Engineering."
*   "It's about structuring your requests to guide the LLM's reasoning process."
*   "Mastering basic prompting patterns is essential for getting the most out of any LLM."
*   **(Resource):** "For more examples, check out resources like AgentRecipes." [[Link: Prompt Chaining Primer]](https://www.agentrecipes.com/prompt-chaining)
-->

*   **Goal:** Get better, more reliable, structured outputs via careful prompting.
*   **Key Patterns** *(Ref Sec 3.2)*:
    *   **Chain-of-Thought (CoT):** Ask model to "think step-by-step" -> improves reasoning. [[Link: CoT Paper]](https://arxiv.org/abs/2201.11903)
    *   **ReAct:** Interleave reasoning & actions (tool use, search). Good for agents. [[Link: ReAct Paper]](https://arxiv.org/abs/2210.03629)
    *   **Self-Critique/Reflexion:** Prompt model to review & revise its output. [[Link: Reflexion Paper]](https://arxiv.org/abs/2303.11366)
    *   **Skeleton-of-Thought (SoT):** Generate outline first, then elaborate. Good for writing. [[Link: SoT Paper]](https://arxiv.org/abs/2307.15337)

---

# How Models Are Built: The Training Pipeline

<!-- _comment:
*   "So we know how to *use* these models better with RAG and prompting, but how do they get so capable in the first place? It's typically a multi-stage process." *(Referencing Section 4.1 Table)*
*   "Understanding this pipeline helps appreciate why models behave the way they do and what goes into creating or customizing them."
*   "This pipeline transforms a raw 'next-word predictor' into the sophisticated, helpful assistants we interact with."
*   **(Visual Aid):** *(Simple diagram: Internet Data -> [Pre-training] -> Base Model -> Labeled Data -> [Fine-tuning] -> Instruction-Tuned Model -> Human Preferences -> [Alignment] -> Aligned/Helpful Model)*
-->

*   **Typical 3-Stage Pipeline:**
    *   **1. Pre-training:** Learns general language/knowledge from massive unlabeled data (trillions of tokens) via next-word prediction. Most compute-intensive.
    *   **2. Fine-tuning (SFT):** Adapts base model for specific tasks using smaller, labeled datasets (e.g., Q&A pairs, instructions). LoRA/QLoRA improve efficiency.
    *   **3. Alignment (RLHF/DPO):** Makes model helpful, harmless, honest using human feedback (RLHF) or newer methods (DPO) to guide outputs.

---

# Meet the Frontier Models (1/2)

<!-- _comment:
*   "Having covered how models are built and used, let's briefly look at the 'Formula 1' of AI – the frontier models pushing the boundaries right now." *(Referencing Section 5.1 Table)*
*   "These models represent the cutting edge in terms of reasoning, context length, and multimodality."
*   "This landscape changes fast! Key things to watch are reasoning improvements, context window expansion (like Gemini's 1M tokens), multimodality, and efficiency (cost/speed like the 'o' series)."
*   *(Optional: Mention leading open-weight models like Llama-3 70B or Mixtral 8x22B as strong alternatives, referencing the full table).*
*   Links: [[GPT-4o]](https://openai.com/index/gpt-4o-system-card/) [[o3/o4]](https://openai.com/index/introducing-o3-and-o4-mini/) [[GPT-4.1]](https://platform.openai.com/docs/models#gpt-4.1) [[GPT-4.5]](https://openai.com/index/introducing-gpt-4-5/) [[Claude 3.7]](https://www.anthropic.com/news/claude-3-7-sonnet)
-->

*   **Highlighting Key Players & Models:**
    *   **OpenAI:**
        *   `GPT-4o`: Fast, multimodal default.
        *   `o3 / o4-mini / o4-mini-high`: Cost-optimized frontier performance.
        *   `GPT-4.1`: Latest API preview, peak reasoning.
        *   `GPT-4.5 "Orion"`: Research preview, top benchmarks.
    *   **Anthropic:**
        *   `Claude 3.7 Sonnet`: Excels in coding, long writing, STEM; 200k context; Hybrid arch.

---

# Meet the Frontier Models (2/2)

<!-- _comment:
NEW: Including the latest open-weight models is crucial, as they are rapidly closing the gap with proprietary models. DeepSeek V3, Qwen 2.5-1M, and Llama-3 70B are all state-of-the-art, open, and commercially usable. These models are not just for hobbyists—many startups and researchers use them for real-world applications, and they often serve as the backbone for local/private deployments and custom fine-tuning.
-->

**Highlighting Key Players & Models (Cont.):**

- **Google**
    - `Gemini 2.5 Flash`: Cost-effective MoE, 1M token context.
    - `Gemini 2.5 Pro Preview`: 1M context, enhanced reasoning.
- **Meta**
    - `Llama-3 70B`: Open-weight, strong coding, commercial use.
- **DeepSeek**
    - `DeepSeek V3`: +50% reasoning vs V2, top open weight benchmarks.
- **Alibaba**
    - `Qwen 2.5-1M`: 1M token context, MoE, open-source first.

---

# The AI Ecosystem: Tools of the Trade

<!-- _comment:
*   "Alright, we know the models and techniques. Now, let's look at the tools and platforms that bring AI capabilities to users and developers." *(Transition)*
*   "This ecosystem is vast and growing daily, but we'll highlight key categories."
*   "First and foremost you're gonna want to get some API keys. The easiest way to get a variety of keys is through single source. One of my favorites is Open Router, so I'll recommend that for now, Though other services are always popping up."
-->

*   Focus on practical tools for Research, Coding, Agents, and Creativity.

 https://openrouter.ai/models
---

# AI Search Engines & Research Tools

<!-- _comment:
*   "Standard search is being revolutionized by AI. Engines like Perplexity, Google Gemini, ChatGPT, and DeepSeek offer conversational search and synthesis." *(Referencing Section 6.2 Table)*
*   "These are invaluable for research, learning, and getting up to speed on new domains quickly."

-->
*   Links: [[Perplexity]](https://www.perplexity.ai) [[Gemini]](https://gemini.google.com) [[DeepSeek]](https://deepseek.com)
*   **Beyond Keywords:** Understand intent, summarize, cite, allow follow-ups.
*   **Deep Research Modes:** Many offer multi-step investigation, source analysis, report generation (Perplexity, Gemini Advanced, ChatGPT agent). Significant leap beyond simple Q&A.

---

# AI-Infused Coding Tools & IDEs

<!-- _comment:
*   "AI is also transforming software development. Tools range from autocompletion to full-fledged coding agents." *(Referencing Section 6.3 Table)*
*   "These tools significantly boost developer productivity but require careful oversight."

-->
*   Links: [[Cursor]](https://www.cursor.com) [[v0]](https://v0.dev) [[Continue]](https://www.continue.dev) [[Cline]](https://cline.bot) [[Roo Code]](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-code) [[Warp Terminal]](https://warp.dev)
*   **Spectrum of Assistance:**
    *   **Autocompletes:** GitHub Copilot, Tabby (self-hosted).
    *   **Chat/Debug Assistants:** Integrated chat in VS Code, Cursor, JetBrains AI.
    *   **Context-Aware IDEs:** Cursor builds the IDE *around* AI interaction.
    *   **One-Shot Agents:** Vercel v0 (UI generation), Replit AI (scaffolding).
    *   **Advanced Plugins:** Continue, **Cline** (autonomous tasks). Explore **MCP (Model Context Protocol)** for better context sharing between agents.

---

# Agent Frameworks & Orchestrators

<!-- _comment:
*   "Beyond single tools, there are frameworks for building more complex AI *agents* – systems that can reason, plan, and use multiple tools to achieve goals." *(Referencing Section 6.6 Table)*
*   "These frameworks are key for building sophisticated autonomous systems, moving beyond simple chatbots."
*   Links: [[LangChain]](https://github.com/langchain-ai/langchain) [[LlamaIndex]](https://github.com/run-llama/llama_index) [[AutoGen]](https://github.com/microsoft/autogen) [[CrewAI]](https://github.com/joaomdmoura/crewAI)
-->

*   **Core Libraries:**
    *   **LangChain:** Popular, versatile for chains, tools, agents. Can be complex.
    *   **LlamaIndex:** Focuses on indexing data for effective RAG. Used *with* LangChain.
*   **Multi-Agent Frameworks:**
    *   **AutoGen (Microsoft):** Collaborative agents (e.g., coder + tester).
    *   **CrewAI:** Role-playing agents with delegation.

---

# Desktop Clients & Local Model Runners

<!-- _comment:
*   "While many interact with AI via web interfaces, dedicated desktop apps offer more features and the ability to run models *locally*." *(Referencing Section 6.4 Table)*
*   "Running models locally gives you more control and avoids API costs, but requires capable hardware."
*   Links: [[ChatGPT Desktop]](https://openai.com/chatgpt/desktop/) [[Claude Desktop]](https://support.anthropic.com/en/articles/10065433-installing-claude-for-desktop) [[LM Studio]](https://lmstudio.ai) [[AnythingLLM]](https://useanything.com)
-->

*   **Official Apps:** ChatGPT & Claude desktops offer better integration (hotkeys, file uploads).
*   **Local Runners:** **LM Studio**, **AnythingLLM** let you download & run open-source models (Llama 3, Mixtral) locally. Good for privacy, offline use, experimentation.

---

# AI Creative Suite & Open Source Tools

<!-- _comment:
*   "AI is also a powerful creative partner. We already mentioned MidJourney and RunwayML." *(Referencing Section 6.5 Table)*
*   "These tools are democratizing content creation across various media."
*   Link: [[ComfyUI]](https://github.com/comfyanonymous/ComfyUI)
-->
[1](https://www.midjourney.com), [2](https://www.krea.ai), [3](https://ideogram.ai), [4](https://openai.com/dall-e-3), [5](https://runwayml.com), [6](https://suno.com), [7](https://www.udio.com), [8](https://www.descript.com) 
*   **Key Creative Tools:**
    *   **Image:** MidJourney, Krea, Ideogram, DALL-E 3.
    *   **Video:** RunwayML (Gen-3/4), others.
    *   **Audio/Music:** Suno, Udio (text-to-music); Descript (AI editing).
    *   **Workflows:** **ComfyUI** for node-based Stable Diffusion control.

---

# Quick Mention: Web3 x AI Platforms

<!-- _comment:
*   "As highlighted with Andromeda, there's a growing ecosystem specifically focused on the AI and Web3 intersection." *(Referencing Section 6.7 Table)*
*   "This is a rapidly evolving space aiming to make AI development more open, verifiable, and potentially integrated with token economies."
*   Links: [[Fetch.ai]](https://fetch.ai) [[Bittensor]](https://bittensor.com) [[Akash]](https://akash.network) [[Ocean]](https://oceanprotocol.com)
-->

*   **Areas of Focus:**
    *   **On-chain Agents:** Frameworks like **Fetch.ai**; broader trend of autonomous agents interacting with blockchains.
    *   **Decentralized Compute:** Marketplaces like **Bittensor**, **Akash** for distributed AI training/inference.
    *   **Data/Model Marketplaces:** Protocols like **Ocean** for tokenizing AI assets.

---

# Current Applications & Future Horizons

<!-- _comment:
*   "We've covered the tech and the tools. Let's wrap up by looking at some groundbreaking applications happening now and peering into the near future." *(Transition)*
-->

*   Moving beyond general tools to specific, high-impact applications and emerging trends.

---

# AI for Scientific Discovery

<!-- _comment:
*   "One of the most profound impacts of AI is accelerating scientific research."
*   "This collaboration between human researchers and AI promises to tackle some of the world's biggest scientific challenges."
*   Links: [[AlphaFold]](https://www.deepmind.com/research/highlighted-research/alphafold) [[Google AI for Science]](https://deepmind.google/discover/blog/accelerating-science-with-ai/)
-->

*   **AI as Co-scientist:** Building on AlphaFold, AI becomes a lab partner.
*   **Examples:** **Google's Co-scientist** initiatives aim to:
    *   Analyze massive datasets for patterns.
    *   Generate novel hypotheses.
    *   Design experiments / control lab equipment.
    *   Speed up discovery (medicine, materials, climate).

---

# The Convergence of AI & Robotics

<!-- _comment:
*   "Another major frontier is bringing advanced AI into the physical world through robotics."
*   "We're moving from programmed automation to truly intelligent, adaptable robotic systems."
*   Links: [[NVIDIA Isaac]](https://developer.nvidia.com/isaac-sim) [[Covariant]](https://covariant.ai/) [[DeepSeek R1]](https://github.com/deepseek-ai/DeepSeek-R1)
-->

*   **Smarter Robots:** LLMs + vision = better understanding, reasoning, adaptability.
*   **Key Trends:**
    *   **Simulation & Digital Twins:** Train virtual robots (digital twins) before build/deploy (e.g., NVIDIA Isaac Sim). Learn skills safely & quickly.
    *   **Cloud-to-Robot Learning:** Deploy cloud-trained 'brains' onto simpler hardware.
    *   **Autonomous Systems:** Independent operation in dynamic environments (logistics, exploration, household?) (e.g., Covariant, DeepSeek R1).

---

# AI Avatars & The Future of Presence

<!-- _comment:
*   "AI is also changing how we represent ourselves digitally."
*   "This raises fascinating possibilities for productivity and communication, but also significant ethical questions around identity,  deepfakes, and consent that society needs to address."
*   Link: [[ElevenLabs]](https://elevenlabs.io/)
-->

*   **Realistic Digital Likenesses:** Photorealistic avatars + voice cloning (e.g., ElevenLabs) enable AI-driven digital twins.
*   **Potential Applications:**
    *   Automated Meeting Attendance (attend multiple Zooms!).
    *   Personalized video messages at scale.
    *   Interactive virtual assistants / customer service.

---

# Conclusion & Discussion

<!-- _comment:
*   "We've covered a huge amount of ground – from the fundamental Transformer architecture to cutting-edge models, tools, and future applications like AI scientists and autonomous robots."
*   "The pace of change is incredibly fast, but the core principles – understanding the models, using techniques like RAG and prompting effectively, and leveraging the right tools – remain crucial."
*   "The key takeaway is that AI is becoming a general-purpose technology impacting nearly every field. Understanding it is essential."
*   "Thank you for your time. I'd be happy to take any questions you have."
-->

*   Recap: Fundamentals -> Techniques -> Models -> Ecosystem -> Future.
*   AI is a rapidly evolving, general-purpose technology.
*   Understanding core concepts & tools is key.
*   **Questions?**

---

# Connect with Me

**Myron Koch**  
[X (Twitter): @myronkoch](https://x.com/myronkoch)  
[LinkedIn: myronkoch](https://linkedin.com/in/myronkoch)

--- 